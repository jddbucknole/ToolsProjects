---
title: 'Week 6: Machine Learning Intro'
author: "<yournameshere>"
date: "11/23/2020"
output: html_document
---

```{r setup, warnings = F, message = F}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(class)
library(caret)
```

# Part 1: Supervised Learning

Supervised learning is a method of classification or prediction in which we have a labelled set of data from which to make our predictions. Regression (both linear and logistic) are methods of supervised learning as the y values in the 'training' data are know *a priori*.  

This section will focus on creating 3 new models and comparing the output to the regression models we have already seen.  The focus will be on exploration rather than specifics.

## Introduction

For this study, we will examine some simulated data used in the book _Introduction to Statistical Learning in R_ on carseat sales.  Take a look at the data below (the code to view the data is already written).  The response variable of interest is `Sales`.  Make a visualization to see how the sales is related to a predictor or two (you do NOT need to look at all predictors in your viz).

```{r}
carseat <- ISLR::Carseats
glimpse(carseat)
```

Next we need to create a training and test dataset to aid in model evaluation. We will skip the validation set for this exercise.  The code to create the train/test split is already shown below.  The seed is set to make sure that the results are the same every time you knit. The test set will allow us to compare the models based on the results on 'unseen data'

```{r}
set.seed(1234)
train_frac <- .75
index <- sample(1:nrow(carseat), train_frac * nrow(carseat))
train <- carseat[index,]
test <- carseat[-index,]
```



## Regression

### Model 1: Linear Regression

Fit a linear regression model to predict the sales of carseats using whatever predictors you deem useful.  Make sure to FIT the model using the training data only.  This will prevent overfitting.  Once you have settled on a model, predict the results in the test dataset.  

```{r}
deleteme <- 0
```

Now that you have predicted values for the test set, we need to create a metric to evaluate the models efficacy.  For that we will use root mean square error (RMSE).  The `caret` has a function for this (called RMSE) that takes the fitted values and observed values as input.  Note: lower RMSE is better. Save the value as `rmse_lm`.

```{r}
deleteme <- 0
```

*Write a few sentences to describe the model including the significant variables and their effect on the response*



### Model 2: Regression Trees

Regression trees simply create a series of decision rules based on the predictors and average the results in the training set to create the prediction.  I have written the code in the next block to fit this model AND visualize it.  Please examine the plot as I think it is quite intuitive in it's interpretation.

```{r}
regtreemodel <- rpart(Sales ~ ., data = train)
rpart.plot(regtreemodel)
```

You should note that the tree is quite busy trying to make the best prediction on the `train` set.  This can easily lead to overfitting.  To fix this, we can `prune` the tree.  This will remove the splits that are not as valuable.  There is a lot of detail being skipped here, but there are resources galore to decribe what is happening.  Below, you will find the code to prune and plot the pruned model. 

```{r}
pruned_regtree <- prune(regtreemodel, cp = .075)
rpart.plot(pruned_regtree)
```
Notice that the pruned tree is MUCH simpler.  Once again, the solution makes predictions based on averaging the sales of the observations in the training set that end at that terminal node.  

Add code to use the pruned tree to predict on the test set (same function as prediction elsewhere) and calculate the RMSE for this model (saved as rmse_regtree).

```{r}
deleteme <- 0
```

**Write a few lines interpreting the tree model (which predictors matter and how).  In addition, write a sentence comparing the results or the linear regression vs. the regression tree. Which do you prefer?**

## Classification Methods

So far, we have focused on predicting a numerical output, but what if we want to predict a categorical response.  We've actually already did this with logistic regression.  While there are many metrics to evaluate the efficacy of classification algorithms, we will examine the confusion matrices (`confusionMatrix` function in the caret package to see how the models perform).

Since our output is numeric, we need to create a categorical representation of the response.  For this, we will try to predict the occurance of a large amount of sales.  If sales are over 10, we will consider this a success.  Create a new variable in the dataset that is 1 if sales exceed 10 and 0 otherwise.  Use `mutate` and `ifelse` to accomplish this.

```{r}
deleteme <- 0
```

### Model 3: Logistic regression

1. Fit a logistic regression model predicting the probability of a 'success' using all predictor variables EXCEPT Sales. 
2. Choose which predictors you think are significant in your final model.
3. Predict the 'response' for the test data.
4. Convert the predictions to 1 if probability is greater than 0.5 and 0 otherwise.
5. Use the confusionMatrix function 

