---
title: 'Week 13: Unsupervised Machine Learning'
author: "<yournameshere>"
output: html_document
---

```{r setup, warning = F, message = F}
library(tidyverse)
library(cluster)
library(dendextend)
library(ISLR)
```


# Unsupervised Learning

Unsupervised learning is such that the data is not labelled *a priori*.  The goal is simply to find groups or clusters within the data based on similarities in the variables.  Distance metrics are key here as in `knn` but the fundamental difference is that clustering doesn't have predetermined groups.  In fact, we don't even know how many groups exist!  Market segmentation is a perfect example of this phenomenon.  Marketers seek to find similarities in their customers to (hopefully) target groups of people in similar ways.

## Method 1: k-means clustering

The k-means clustering algorithm contains steps that iterate

1. Choose k random points to be the initial class centroids.
2. Assign each point in the dataset to the class with the nearest centroid.
3. Calculate the average of all points ina given class.  This is the new class centroid.
4. Repeat steps 1-3 until convergence.

The following [video](https://www.youtube.com/watch?v=IuRb3y8qKX4) shows this process.  The best part is that all of this happens under the hood and the function is simple!

For our example, we will use the College data in the ISLR package to find 3 clusters within the colleges.  As with `knn` the data needs to be numeric and normalized.  The data is loaded and preprocessed below.  Add the code to run the kmeans algorithm.  The function is `kmeans()` which takes the data and the number of clusters you desire.  The output is the cluster label for each point.  To unify the results, I have set the random seed.  This will assure cluster 1 stays 'cluster 1'.

```{r}
normalize <- function(x){
  (x - min(x))/(max(x) - min(x))
}


college <- College %>%
  mutate(Private = ifelse(Private == "Yes", 1, 0)) %>%
  mutate_all(normalize)

set.seed(1234)
#Your kmeans code (Save the results in a variable called clusters)

```

Examine the different clusters by looking through dataset.  Do you see any ways to 'define' the clusters in a contextual sense? What 'type' of school falls into cluster 1, 2, or 3? This is the art of data science!

### Hierarchical Clustering 

Hierarchical clustering creates clusters by one of two approaches:

1) Agglomerative: bottom up/all points start as their own group, merge closest groups until all clusters are joined.
2) Divisive: top down/Start with all points in one cluster and break them apart.

We will focus on agglomerative clustering approach.

The algorithm is as follows:

1) Scale the data (similar to kmeans).
2) Calculate the distance between all clusters (using any defined distance metric)
3) Merge clusters with the smallest intercluster distance (could use complete, single, average or maximal linkage)
4) Iterate steps 2 and 3 until there is one cluster.

The output is then visualized as a dendrogram (tree-like diagram).  From the dendrogram, you can choose the appropriate number of clusters.  It is recommended to choose the 'cut' between the splits of greatest distance.

Let's repeat the analysis of the `College` data using hierarchical clustering.  Run the analysis using complete linkage and cut the tree at 3 clusters (show a dendrogram with colored branches).  



```{r}
deleteme <- 1
```

Now compare your results to the kmeans algorithm.  You should have 3 clusters from kmeans and 3 from hierarchical clustering.  Make a table (using the `table()` function) that compares the clustering results.  Do they match up? On what type of school do they differ?

```{r}
deleteme <- 1
```

# If time permits

Run the kmeans() algorithm for a different number of clusters (k=1:10) and plot the withinss for each k vs. k. Find where the plot hits an `elbow` to choose the optimal number of clusters.

OR

Try using different linkages for the hierarchical clustering and note how the clustering changes.


